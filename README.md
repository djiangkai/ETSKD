# ETSKD
![image](https://github.com/user-attachments/assets/1b7e54d7-90a3-4db8-aa00-abf7482a4a8c)

Parameter Settings (such as α, β)
α: This hyperparameter balances the contribution of the cross - entropy (CE) loss and the KL - divergence loss between the auxiliary branches' logits and the elite teacher's logits. As shown in our ablation study on ResNet - 18 in Table 6, when α is large, the LSKD loss term becomes too small, and network performance decreases significantly. We found that α = 0.3 yields the best performance on the CIFAR - 100 dataset for ResNet - 18. For other datasets and network architectures, we suggest researchers start with α = 0.3 and then perform a small - scale grid search (e.g., try α values in the range of [0.2, 0.4] with a step of 0.05) to find the optimal value. The optimal α may vary depending on the complexity of the network, the size of the dataset, and the nature of the classification task.
β: This hyperparameter controls the importance of the SFFM - integrated FSKD loss term. From Figure 5, we can see that when β = 0.01, the ResNet - 18 model on the CIFAR - 100 dataset achieves the best performance. Similar to α, for different datasets and networks, we recommend starting with β = 0.01 and then fine - tuning it in a small range (e.g., [0.005, 0.015] with a step of 0.002) to optimize the model's performance. If the dataset has complex features or the network has difficulty learning discriminative features, increasing β slightly may help the network better capture the spatial - frequency fusion features. Conversely, if the network is overfitting due to the FSKD loss, decreasing β can be considered.
